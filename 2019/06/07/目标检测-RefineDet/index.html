
<!DOCTYPE html>
<html lang class="loading">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>目标检测-RefineDet - 子铭 の blog</title>
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate">
    <meta name="keywords" content="zim,"> 
    <meta name="description" content="refineDetmotivationone stage目标检测准确率低的原因是因为 class imbalance problem。
已有的一些解决方案｛

object prior constr,"> 
    <meta name="author" content="ziming"> 
    <link rel="alternative" href="atom.xml" title="子铭 の blog" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <link rel="stylesheet" href="/css/diaspora.css">
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({
              google_ad_client: "ca-pub-8691406134231910",
              enable_page_level_ads: true
         });
    </script>
    <script async custom-element="amp-auto-ads" src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
</head>
</html>
<body class="loading">
    <span id="config-title" style="display:none">子铭 の blog</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="icon-home image-icon" href="javascript:;" data-url="http://cvcv.me"></a>
    <div title="播放/暂停" class="icon-play"></div>
    <h3 class="subtitle">目标检测-RefineDet</h3>
    <div class="social">
        <!--<div class="like-icon">-->
            <!--<a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
        <!--</div>-->
        <div>
            <div class="share">
                <a title="获取二维码" class="icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class="main">
        <h1 class="title">目标检测-RefineDet</h1>
        <div class="stuff">
            <span>六月 07, 2019</span>
            
  <ul class="post-tags-list"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/目标检测/">目标检测</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/论文/">论文</a></li></ul>


        </div>
        <div class="content markdown">
            <h2 id="refineDet"><a href="#refineDet" class="headerlink" title="refineDet"></a>refineDet</h2><h3 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h3><p><code>one stage</code>目标检测准确率低的原因是因为 <code>class imbalance problem</code>。</p>
<p>已有的一些解决方案｛</p>
<ul>
<li>object prior constraint[24]</li>
<li>reshaping the standard cross entropy loss to focus on hard examples[28]</li>
<li>max-out labeling mechanism[53]</li>
</ul>
<p>｝</p>
<p><code>two stage</code> 工作的advantages｛</p>
<ul>
<li>two stage 结构做 sampling heuristics 采样探索 来处理 class imbalance problem</li>
<li>两步级联地去回归 object box parameters</li>
<li>two stage features 来描述objects–&gt; RPN 特征 预测二分类（object / background）， stage2 预测 多分类（BG and objects classes）  <strong>本文借鉴了这种两次分类的好处</strong></li>
</ul>
<p>｝</p>
<h3 id="structure-两个模块"><a href="#structure-两个模块" class="headerlink" title="structure 两个模块"></a>structure 两个模块</h3><ul>
<li>anchor refinement module ARM <strong>*</strong></li>
<li>objection detection module  ODM</li>
</ul>
<p><img src="https://github.com/ziming-liu/BLOG/blob/master/object%20detection/refineDet/src/Snipaste_2019-05-13_16-18-43.png?raw=true" alt="Snipaste_2019-05-13_16-18-43.png"></p>
<p>整个模型的结构可以分解为三部分 </p>
<ul>
<li>backbone                                                                                ———&gt; Features[F1,F2,F3,F4]</li>
<li>necks  包括  FPN（blue path ） +  ARM（red path）       ———&gt;refine anchors[A1,…A4] +FPNfeatures[F1’….F4’] </li>
<li>head (SSD head 或者retina head)                                       ———–&gt;predict multi classes + bbox loc</li>
</ul>
<p>为了方便理解，我们可以把ARM模块化成红色的path， 把FPN 特征金字塔化成蓝色的path。这样理解， <code>refineDet</code>的<code>anchor refine</code> 就比较好理解成一个module了.</p>
<p>ARM 实际上很像two stage 模型里RPN 承担的作用，paper中也说是在<code>imitate</code> two stage模型</p>
<h3 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h3><ul>
<li>采用<code>jaccard overlap</code>[7] 对应anchors和GTbox， 1  match给每个gtbox 一个与它overlap最大的anchor， 2 match所有anchor boxs 到 与其overlap 大于0.5 的gtbox。</li>
<li>anchor 选择的正负样本比例  nagtives:positives==3:1  ，选择 loss值大的 负样本anchors，而不是选择全部负样本anchor或者随机选择负样本anchor</li>
</ul>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/luuuyi/RefineDet.PyTorch" target="_blank" rel="noopener">https://github.com/luuuyi/RefineDet.PyTorch</a></p>
<p>模型代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#main model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RefineDet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Single Shot Multibox Architecture</span></span><br><span class="line"><span class="string">    The network is composed of a base VGG network followed by the</span></span><br><span class="line"><span class="string">    added multibox conv layers.  Each multibox layer branches into</span></span><br><span class="line"><span class="string">        1) conv2d for class conf scores</span></span><br><span class="line"><span class="string">        2) conv2d for localization predictions</span></span><br><span class="line"><span class="string">        3) associated priorbox layer to produce default bounding</span></span><br><span class="line"><span class="string">           boxes specific to the layer's feature map size.</span></span><br><span class="line"><span class="string">    See: https://arxiv.org/pdf/1512.02325.pdf for more details.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        phase: (string) Can be "test" or "train"</span></span><br><span class="line"><span class="string">        size: input image size</span></span><br><span class="line"><span class="string">        base: VGG16 layers for input, size of either 300 or 500</span></span><br><span class="line"><span class="string">        extras: extra layers that feed to multibox loc and conf layers</span></span><br><span class="line"><span class="string">        head: "multibox head" consists of loc and conf conv layers</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, phase, size, base, extras, ARM, ODM, TCB, num_classes)</span>:</span></span><br><span class="line">        super(RefineDet, self).__init__()</span><br><span class="line">        self.phase = phase</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.cfg = (coco_refinedet, voc_refinedet)[num_classes == <span class="number">21</span>]</span><br><span class="line">        self.priorbox = PriorBox(self.cfg[str(size)])</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            self.priors = self.priorbox.forward()</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># SSD network</span></span><br><span class="line">        self.vgg = nn.ModuleList(base)</span><br><span class="line">        <span class="comment"># Layer learns to scale the l2 normalized features from conv4_3</span></span><br><span class="line">        self.conv4_3_L2Norm = L2Norm(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">        self.conv5_3_L2Norm = L2Norm(<span class="number">512</span>, <span class="number">8</span>)</span><br><span class="line">        self.extras = nn.ModuleList(extras)</span><br><span class="line"></span><br><span class="line">        self.arm_loc = nn.ModuleList(ARM[<span class="number">0</span>])</span><br><span class="line">        self.arm_conf = nn.ModuleList(ARM[<span class="number">1</span>])</span><br><span class="line">        self.odm_loc = nn.ModuleList(ODM[<span class="number">0</span>])</span><br><span class="line">        self.odm_conf = nn.ModuleList(ODM[<span class="number">1</span>])</span><br><span class="line">        <span class="comment">#self.tcb = nn.ModuleList(TCB)</span></span><br><span class="line">        self.tcb0 = nn.ModuleList(TCB[<span class="number">0</span>])</span><br><span class="line">        self.tcb1 = nn.ModuleList(TCB[<span class="number">1</span>])</span><br><span class="line">        self.tcb2 = nn.ModuleList(TCB[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> phase == <span class="string">'test'</span>:</span><br><span class="line">            self.softmax = nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">            self.detect = Detect_RefineDet(num_classes, self.size, <span class="number">0</span>, <span class="number">1000</span>, <span class="number">0.01</span>, <span class="number">0.45</span>, <span class="number">0.01</span>, <span class="number">500</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""Applies network layers and ops on input image(s) x.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input image or batch of images. Shape: [batch,3,300,300].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            Depending on phase:</span></span><br><span class="line"><span class="string">            test:</span></span><br><span class="line"><span class="string">                Variable(tensor) of output class label predictions,</span></span><br><span class="line"><span class="string">                confidence score, and corresponding location predictions for</span></span><br><span class="line"><span class="string">                each object detected. Shape: [batch,topk,7]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            train:</span></span><br><span class="line"><span class="string">                list of concat outputs from:</span></span><br><span class="line"><span class="string">                    1: confidence layers, Shape: [batch*num_priors,num_classes]</span></span><br><span class="line"><span class="string">                    2: localization layers, Shape: [batch,num_priors*4]</span></span><br><span class="line"><span class="string">                    3: priorbox layers, Shape: [2,num_priors*4]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        sources = list()</span><br><span class="line">        tcb_source = list()</span><br><span class="line">        arm_loc = list()</span><br><span class="line">        arm_conf = list()</span><br><span class="line">        odm_loc = list()</span><br><span class="line">        odm_conf = list()</span><br><span class="line">-------------------<span class="number">-1</span>------------------------------------------------</span><br><span class="line">        <span class="comment"># apply vgg up to conv4_3 relu and conv5_3 relu</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">            x = self.vgg[k](x)</span><br><span class="line">            <span class="keyword">if</span> <span class="number">22</span> == k:</span><br><span class="line">                s = self.conv4_3_L2Norm(x)</span><br><span class="line">                sources.append(s)</span><br><span class="line">            <span class="keyword">elif</span> <span class="number">29</span> == k:</span><br><span class="line">                s = self.conv5_3_L2Norm(x)</span><br><span class="line">                sources.append(s)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply vgg up to fc7</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">30</span>, len(self.vgg)):</span><br><span class="line">            x = self.vgg[k](x)</span><br><span class="line">        sources.append(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply extra layers and cache source layer outputs</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> enumerate(self.extras):</span><br><span class="line">            x = F.relu(v(x), inplace=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">if</span> k % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">                sources.append(x)</span><br><span class="line">-----------------------------------------------------------------</span><br><span class="line">-----------------------<span class="number">-2</span>-----------------------------------------------</span><br><span class="line">        <span class="comment"># apply ARM and ODM to source layers</span></span><br><span class="line">        <span class="keyword">for</span> (x, l, c) <span class="keyword">in</span> zip(sources, self.arm_loc, self.arm_conf):</span><br><span class="line">            arm_loc.append(l(x).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous())</span><br><span class="line">            arm_conf.append(c(x).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous())</span><br><span class="line">        arm_loc = torch.cat([o.view(o.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="keyword">for</span> o <span class="keyword">in</span> arm_loc], <span class="number">1</span>)</span><br><span class="line">        arm_conf = torch.cat([o.view(o.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="keyword">for</span> o <span class="keyword">in</span> arm_conf], <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print([x.size() for x in sources])</span></span><br><span class="line">        <span class="comment"># calculate TCB features</span></span><br><span class="line">        <span class="comment">#print([x.size() for x in sources])</span></span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">----------------------<span class="number">-3</span>----------------------------------------------</span><br><span class="line">        p = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> enumerate(sources[::<span class="number">-1</span>]):</span><br><span class="line">            s = v</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">                s = self.tcb0[(<span class="number">3</span>-k)*<span class="number">3</span> + i](s)</span><br><span class="line">                <span class="comment">#print(s.size())</span></span><br><span class="line">            <span class="keyword">if</span> k != <span class="number">0</span>:</span><br><span class="line">                u = p</span><br><span class="line">                u = self.tcb1[<span class="number">3</span>-k](u)</span><br><span class="line">                s += u</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">                s = self.tcb2[(<span class="number">3</span>-k)*<span class="number">3</span> + i](s)</span><br><span class="line">            p = s</span><br><span class="line">            tcb_source.append(s)</span><br><span class="line">        <span class="comment">#print([x.size() for x in tcb_source])</span></span><br><span class="line">        tcb_source.reverse()</span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">-----------------------<span class="number">-4</span>---------------------------------------------</span><br><span class="line">        <span class="comment"># apply ODM to source layers</span></span><br><span class="line">        <span class="keyword">for</span> (x, l, c) <span class="keyword">in</span> zip(tcb_source, self.odm_loc, self.odm_conf):</span><br><span class="line">            odm_loc.append(l(x).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous())</span><br><span class="line">            odm_conf.append(c(x).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous())</span><br><span class="line">        odm_loc = torch.cat([o.view(o.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="keyword">for</span> o <span class="keyword">in</span> odm_loc], <span class="number">1</span>)</span><br><span class="line">        odm_conf = torch.cat([o.view(o.size(<span class="number">0</span>), <span class="number">-1</span>) <span class="keyword">for</span> o <span class="keyword">in</span> odm_conf], <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print(arm_loc.size(), arm_conf.size(), odm_loc.size(), odm_conf.size())</span></span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">-----------------------------<span class="number">-5</span>-------------------------------------</span><br><span class="line">        <span class="keyword">if</span> self.phase == <span class="string">"test"</span>:</span><br><span class="line">            <span class="comment">#print(loc, conf)</span></span><br><span class="line">            output = self.detect(</span><br><span class="line">                arm_loc.view(arm_loc.size(<span class="number">0</span>), <span class="number">-1</span>, <span class="number">4</span>),           <span class="comment"># arm loc preds</span></span><br><span class="line">                self.softmax(arm_conf.view(arm_conf.size(<span class="number">0</span>), <span class="number">-1</span>,</span><br><span class="line">                             <span class="number">2</span>)),                               <span class="comment"># arm conf preds</span></span><br><span class="line">                odm_loc.view(odm_loc.size(<span class="number">0</span>), <span class="number">-1</span>, <span class="number">4</span>),           <span class="comment"># odm loc preds</span></span><br><span class="line">                self.softmax(odm_conf.view(odm_conf.size(<span class="number">0</span>), <span class="number">-1</span>,</span><br><span class="line">                             self.num_classes)),                <span class="comment"># odm conf preds</span></span><br><span class="line">                self.priors.type(type(x.data))                  <span class="comment"># default boxes</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output = (</span><br><span class="line">                arm_loc.view(arm_loc.size(<span class="number">0</span>), <span class="number">-1</span>, <span class="number">4</span>),</span><br><span class="line">                arm_conf.view(arm_conf.size(<span class="number">0</span>), <span class="number">-1</span>, <span class="number">2</span>),</span><br><span class="line">                odm_loc.view(odm_loc.size(<span class="number">0</span>), <span class="number">-1</span>, <span class="number">4</span>),</span><br><span class="line">                odm_conf.view(odm_conf.size(<span class="number">0</span>), <span class="number">-1</span>, self.num_classes),</span><br><span class="line">                self.priors</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>主要是forward 方法，简单分成五个part分析。</p>
<p>1- backbone卷积网络提取特征，得到输出应该是四个尺度的features [F1,..F4]</p>
<p>2- ARM模块预测得到bbox的回归和anchor正负样本的二分类结果。，这个结果暂存，arm_loc + arm_conf</p>
<p>3- paper中的TCB 模块从高层特征向底层特征融合，实际上类似FPN特征金字塔，得到新的四个尺度features [F1’…F4’]</p>
<p>4- 第三步的四个尺度特征送入 anchor head ，预测bbox的回归和所有object多类别的分类结果。odm_loc + odm_conf.</p>
<p>5- 如果是test时，把arm_loc + arm_conf    odm_loc + odm_conf. 送入一个detect的类，预测得到最终的bbox。  </p>
<p>解释一下这个过程，在目标检测中我们预测的bbox实际上是一个编码了的长度为4的vector,而不是实际的坐标或者长宽（这一点如果不知道看faster RCNN等paper）。</p>
<p>那么， 在第五步，我们送入detect类完成的就是根据预测的bbox location解码（decode）得到实际的坐标。</p>
<p>首先， anchor refine module(ARM)预测得到arm_loc,  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arm_anchors = decode(arm_Loc,predefined_anchor)</span><br><span class="line"><span class="comment">#predefined_anchor是参数定义的anchors，它没有编码，就是实际坐标值，也就是faster RCNN中说的default  anchors</span></span><br><span class="line"><span class="comment">#arm_loc 是ARM模块预测的、编码的、预测loc</span></span><br><span class="line"><span class="comment">#arm_anchors = 根据 上面两个参数解码出来预测bbox的实际空间坐标值。</span></span><br></pre></td></tr></table></figure>
<p>其次， ODM模块预测得到的odm_loc，<strong>就不是根据事先定义的default anchors去解码了，而是根据刚刚得到的<code>arm_anchors</code> 去解码。</strong></p>
<p><strong>这也就是本文的核心———–&gt; <code>anchor refine</code></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">odm_bbox = decode(odm_loc, arm_anchors)</span><br><span class="line"><span class="comment"># odm_loc是预测的、编码了的 bbox loc</span></span><br><span class="line"><span class="comment"># arm_anchors and odm_bbox都是实际的空间坐标。</span></span><br></pre></td></tr></table></figure>
<p>上面是test阶段，如果是train时，就不需要refine这个anchor啦， 直接把</p>
<p>output = (<br>                arm_loc.view(arm_loc.size(0), -1, 4),<br>                arm_conf.view(arm_conf.size(0), -1, 2),<br>                odm_loc.view(odm_loc.size(0), -1, 4),<br>                odm_conf.view(odm_conf.size(0), -1, self.num_classes),<br>                self.priors<br>            )</p>
<p>送出去计算loss咯， 跟two stage 的模型一样，总的loss 有四项，<code>arm_Loc arm_conf   odm_loc odm_conf</code></p>
<p>其他代码细节，可以参考 <a href="https://github.com/luuuyi/RefineDet.PyTorch" target="_blank" rel="noopener">https://github.com/luuuyi/RefineDet.PyTorch</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line">References</span><br><span class="line">[1] S. Bell, C. L. Zitnick, K. Bala, and R. B. Girshick. Insideoutside net: Detecting objects in context with skip pooling</span><br><span class="line">and recurrent neural networks. In CVPR, pages 2874–2883,</span><br><span class="line">2016. 3, 6, 7, 8</span><br><span class="line">[2] N. Bodla, B. Singh, R. Chellappa, and L. S. Davis. Improving object detection with one line of code. In ICCV, 2017. 7,</span><br><span class="line">8</span><br><span class="line">[3] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A unified</span><br><span class="line">multi-scale deep convolutional neural network for fast object</span><br><span class="line">detection. In ECCV, pages 354–370, 2016. 1, 3</span><br><span class="line">[4] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.</span><br><span class="line">Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. In ICLR, 2015. 4</span><br><span class="line">[5] J. Dai, Y. Li, K. He, and J. Sun. R-FCN: object detection via</span><br><span class="line">region-based fully convolutional networks. In NIPS, pages</span><br><span class="line">379–387, 2016. 1, 3, 6, 7, 8</span><br><span class="line">[6] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei.</span><br><span class="line">Deformable convolutional networks. In ICCV, 2017. 7, 8</span><br><span class="line">[7] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable</span><br><span class="line">object detection using deep neural networks. In CVPR, pages</span><br><span class="line">2155–2162, 2014. 4</span><br><span class="line">[8] M. Everingham, L. J. V. Gool, C. K. I. Williams, J. M. Winn,</span><br><span class="line">and A. Zisserman. The pascal visual object classes (VOC)</span><br><span class="line">challenge. IJCV, 88(2):303–338, 2010. 1, 3</span><br><span class="line">[9] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,</span><br><span class="line">and A. Zisserman. The Leaderboard of the PASCAL</span><br><span class="line">Visual Object Classes Challenge 2012 (VOC2012). http:</span><br><span class="line">//host.robots.ox.ac.uk:8080/leaderboard/</span><br><span class="line">displaylb.php?challengeid=11&amp;compid=4.</span><br><span class="line">Online; accessed 1 October 2017. 8</span><br><span class="line">[10] M. Everingham, L. Van Gool, C. K. I. Williams,</span><br><span class="line">J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http:</span><br><span class="line">//www.pascal-network.org/challenges/VOC/</span><br><span class="line">voc2007/workshop/index.html. Online; accessed</span><br><span class="line">1 October 2017. 2</span><br><span class="line">[11] M. Everingham, L. Van Gool, C. K. I. Williams,</span><br><span class="line">J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http:</span><br><span class="line">//www.pascal-network.org/challenges/VOC/</span><br><span class="line">voc2012/workshop/index.html. Online; accessed</span><br><span class="line">1 October 2017. 2, 3</span><br><span class="line">[12] P. F. Felzenszwalb, R. B. Girshick, D. A. McAllester, and</span><br><span class="line">D. Ramanan. Object detection with discriminatively trained</span><br><span class="line">part-based models. TPAMI, 32(9):1627–1645, 2010. 3</span><br><span class="line">[13] C. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg.</span><br><span class="line">DSSD : Deconvolutional single shot detector. CoRR,</span><br><span class="line">abs/1701.06659, 2017. 3, 5, 6, 7</span><br><span class="line">[14] S. Gidaris and N. Komodakis. Object detection via a multiregion and semantic segmentation-aware CNN model. In</span><br><span class="line">ICCV, pages 1134–1142, 2015. 3, 6</span><br><span class="line">[15] R. B. Girshick. Fast R-CNN. In ICCV, pages 1440–1448,</span><br><span class="line">2015. 1, 3, 5, 6, 7</span><br><span class="line">[16] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich</span><br><span class="line">feature hierarchies for accurate object detection and semantic</span><br><span class="line">segmentation. In CVPR, pages 580–587, 2014. 3</span><br><span class="line">[17] X. Glorot and Y. Bengio. Understanding the difficulty of</span><br><span class="line">training deep feedforward neural networks. In AISTATS,</span><br><span class="line">pages 249–256, 2010. 5</span><br><span class="line">[18] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling</span><br><span class="line">in deep convolutional networks for visual recognition. In</span><br><span class="line">ECCV, pages 346–361, 2014. 3</span><br><span class="line">[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning</span><br><span class="line">for image recognition. In CVPR, pages 770–778, 2016. 3, 4,</span><br><span class="line">7, 8</span><br><span class="line">[20] A. G. Howard. Some improvements on deep convolutional neural network based image classification. CoRR,</span><br><span class="line">abs/1312.5402, 2013. 4</span><br><span class="line">[21] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,</span><br><span class="line">A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and</span><br><span class="line">K. Murphy. Speed/accuracy trade-offs for modern convolutional object detectors. In CVPR, 2017. 5, 7, 8</span><br><span class="line">[22] S. Ioffe and C. Szegedy. Batch normalization: Accelerating</span><br><span class="line">deep network training by reducing internal covariate shift. In</span><br><span class="line">ICML, pages 448–456, 2015. 4</span><br><span class="line">[23] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. B.</span><br><span class="line">Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACMMM,</span><br><span class="line">pages 675–678, 2014. 5</span><br><span class="line">[24] T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, and Y. Chen. RON:</span><br><span class="line">reverse connection with objectness prior networks for object</span><br><span class="line">detection. In CVPR, 2017. 1, 3, 5, 6, 7, 8</span><br><span class="line">[25] T. Kong, A. Yao, Y. Chen, and F. Sun. Hypernet: Towards accurate region proposal generation and joint object detection.</span><br><span class="line">In CVPR, pages 845–853, 2016. 3, 6</span><br><span class="line">[26] H. Lee, S. Eum, and H. Kwon. ME R-CNN: multi-expert</span><br><span class="line">region-based CNN for object detection. In ICCV, 2017. 3</span><br><span class="line">[27] T. Lin, P. Dollar, R. B. Girshick, K. He, B. Hariharan, and ´</span><br><span class="line">S. J. Belongie. Feature pyramid networks for object detection. In CVPR, 2017. 1, 3, 7</span><br><span class="line">[28] T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Dollar. Focal ´</span><br><span class="line">loss for dense object detection. In ICCV, 2017. 1, 3, 7, 8</span><br><span class="line">[29] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft COCO: com- ´</span><br><span class="line">mon objects in context. In ECCV, pages 740–755, 2014. 1,</span><br><span class="line">2, 3, 8</span><br><span class="line">[30] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed,</span><br><span class="line">C. Fu, and A. C. Berg. SSD: single shot multibox detector.</span><br><span class="line">In ECCV, pages 21–37, 2016. 1, 3, 4, 6, 7, 8</span><br><span class="line">[31] W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking</span><br><span class="line">wider to see better. In ICLR workshop, 2016. 4</span><br><span class="line">[32] P. H. O. Pinheiro, R. Collobert, and P. Dollar. Learning to ´</span><br><span class="line">segment object candidates. In NIPS, pages 1990–1998, 2015.</span><br><span class="line">3</span><br><span class="line">[33] P. O. Pinheiro, T. Lin, R. Collobert, and P. Dollar. Learning ´</span><br><span class="line">to refine object segments. In ECCV, pages 75–91, 2016. 3</span><br><span class="line">[34] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi.</span><br><span class="line">You only look once: Unified, real-time object detection. In</span><br><span class="line">CVPR, pages 779–788, 2016. 3, 6</span><br><span class="line">[35] J. Redmon and A. Farhadi. YOLO9000: better, faster,</span><br><span class="line">stronger. CoRR, abs/1612.08242, 2016. 1, 3, 6, 7</span><br><span class="line">[36] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:</span><br><span class="line">towards real-time object detection with region proposal networks. TPAMI, 39(6):1137–1149, 2017. 1, 3, 6, 7, 8</span><br><span class="line">[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,</span><br><span class="line">S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein,</span><br><span class="line">A. C. Berg, and F. Li. Imagenet large scale visual recognition</span><br><span class="line">challenge. IJCV, 115(3):211–252, 2015. 3, 4, 5</span><br><span class="line">[38] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,</span><br><span class="line">and Y. LeCun. Overfeat: Integrated recognition, localization</span><br><span class="line">and detection using convolutional networks. In ICLR, 2014.</span><br><span class="line">3</span><br><span class="line">[39] Z. Shen, Z. Liu, J. Li, Y. Jiang, Y. Chen, and X. Xue. DSOD:</span><br><span class="line">learning deeply supervised object detectors from scratch. In</span><br><span class="line">ICCV, 2017. 3, 6, 8</span><br><span class="line">[40] A. Shrivastava and A. Gupta. Contextual priming and feedback for faster R-CNN. In ECCV, pages 330–348, 2016. 3</span><br><span class="line">[41] A. Shrivastava, A. Gupta, and R. B. Girshick. Training</span><br><span class="line">region-based object detectors with online hard example mining. In CVPR, pages 761–769, 2016. 1, 3, 6, 7, 8</span><br><span class="line">[42] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Beyond skip connections: Top-down modulation for object detection. CoRR, abs/1612.06851, 2016. 3, 7, 8</span><br><span class="line">[43] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR,</span><br><span class="line">abs/1409.1556, 2014. 3, 4</span><br><span class="line">[44] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.</span><br><span class="line">Inception-v4, inception-resnet and the impact of residual</span><br><span class="line">connections on learning. In AAAI, pages 4278–4284, 2017.</span><br><span class="line">4, 7</span><br><span class="line">[45] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,</span><br><span class="line">D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.</span><br><span class="line">Going deeper with convolutions. In CVPR, pages 1–9, 2015.</span><br><span class="line">6</span><br><span class="line">[46] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and</span><br><span class="line">A. W. M. Smeulders. Selective search for object recognition.</span><br><span class="line">IJCV, 104(2):154–171, 2013. 3</span><br><span class="line">[47] P. A. Viola and M. J. Jones. Rapid object detection using a</span><br><span class="line">boosted cascade of simple features. In CVPR, pages 511–</span><br><span class="line">518, 2001. 3</span><br><span class="line">[48] X. Wang, A. Shrivastava, and A. Gupta. A-fast-rcnn: Hard</span><br><span class="line">positive generation via adversary for object detection. In</span><br><span class="line">CVPR, 2017. 3</span><br><span class="line">[49] S. Xie, R. B. Girshick, P. Dollar, Z. Tu, and K. He. Aggre- ´</span><br><span class="line">gated residual transformations for deep neural networks. In</span><br><span class="line">CVPR, 2017. 4, 8</span><br><span class="line">[50] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang. Gated</span><br><span class="line">bi-directional CNN for object detection. In ECCV, pages</span><br><span class="line">354–369, 2016. 3</span><br><span class="line">[51] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li. Detecting face with densely connected face proposal network.</span><br><span class="line">In CCBR, pages 3–12, 2017. 4</span><br><span class="line">[52] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.</span><br><span class="line">Faceboxes: A CPU real-time face detector with high accuracy. In IJCB, 2017. 4</span><br><span class="line">[53] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.</span><br><span class="line">S</span><br><span class="line">3</span><br><span class="line">FD: Single shot scale-invariant face detector. In ICCV,</span><br><span class="line">2017. 1, 3, 4</span><br><span class="line">[54] Y. Zhu, C. Zhao, J. Wang, X. Zhao, Y. Wu, and H. Lu. Couplenet: Coupling global structure with local parts for object</span><br><span class="line">detection. In ICCV, 2017. 3, 5, 6, 7</span><br></pre></td></tr></table></figure>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src>
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        <li title="0" data-url="http://music.163.com/song/media/outer/url?id=4377971.mp3"></li>
                    
                        <li title="1" data-url="http://music.163.com/song/media/outer/url?id=2866895.mp3"></li>
                    
                </ul>
            
        </div>
        
    <div id="gitalk-container" class="comment link" data-ae="true" data-ci="9afa24e4e91c735a0e03" data-cs="7b3dbe48c62e25dc29d079a3e7d950e4251ea9f7" data-r="ziming-liu.github.io" data-o="ziming-liu" data-a="ziming-liu" data-d="true">查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/diaspora.js"></script>
<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">
<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>




</html>
